{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 4 prompts in 39.48 seconds with a batch size of 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 4 prompts in 39.49 seconds with a batch size of 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 4 prompts in 39.56 seconds with a batch size of 4\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np  # Assuming numerical operations might be needed\n",
    "from models.model_manager import ModelManager \n",
    "\n",
    "# class ModelManager:\n",
    "#     def __init__(self, model_path):\n",
    "#         # Dummy initialization - replace with actual model loading logic\n",
    "#         self.model_path = model_path\n",
    "    \n",
    "#     def infer(self, texts):\n",
    "#         # Dummy inference - simulate processing time and return dummy results\n",
    "#         time.sleep(0.01 * len(texts))  # Simulate inference time scaling with batch size\n",
    "#         return [\"Result for: \" + text for text in texts]\n",
    "\n",
    "def load_data(file_path):\n",
    "    \"\"\" Load data from a CSV file assuming there's a column 'prompt' for input data. \"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    return df['prompt'].tolist()\n",
    "\n",
    "def process_batches(prompts, model, batch_size):\n",
    "    \"\"\" Process data in batches and collect results. \"\"\"\n",
    "    results = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i in range(0, len(prompts), batch_size):\n",
    "        batch = prompts[i:i + batch_size]\n",
    "        batch_results = model.infer(batch)\n",
    "        results.extend(batch_results)\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"Processed {len(prompts)} prompts in {total_time:.2f} seconds with a batch size of {batch_size}\")\n",
    "    return results\n",
    "\n",
    "def main():\n",
    "    data_path = 'test13.csv'  # Path to your CSV file\n",
    "    model_path = '/home/peiman/projects/RayInference/gpt-2'  # Path to your model, adjust accordingly\n",
    "\n",
    "    # Initialize model\n",
    "    model = ModelManager(model_path)\n",
    "\n",
    "    # Load data\n",
    "    prompts = load_data(data_path)\n",
    "\n",
    "    # Define different batch sizes to test\n",
    "    batch_sizes = [1, 2, 4]\n",
    "\n",
    "    # Experiment with different batch sizes\n",
    "    for batch_size in batch_sizes:\n",
    "        results = process_batches(prompts, model, batch_size)\n",
    "        # Optionally, save results to file or further analyze them\n",
    "        # For simplicity, this example just prints out processing information\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peiman/miniconda3/envs/ray/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  2.22it/s]\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "/home/peiman/miniconda3/envs/ray/lib/python3.9/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of size 1 in 1.17 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peiman/miniconda3/envs/ray/lib/python3.9/site-packages/transformers/generation/utils.py:1178: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of size 1 in 0.28 seconds.\n",
      "Processed batch of size 1 in 0.24 seconds.\n",
      "Processed batch of size 1 in 0.22 seconds.\n",
      "Processed batch of size 1 in 0.02 seconds.\n",
      "Processed batch of size 1 in 0.28 seconds.\n",
      "Processed batch of size 1 in 0.29 seconds.\n",
      "Processed batch of size 1 in 0.02 seconds.\n",
      "Processed batch of size 1 in 0.29 seconds.\n",
      "Processed batch of size 1 in 0.30 seconds.\n",
      "Processed batch of size 1 in 0.02 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peiman/miniconda3/envs/ray/lib/python3.9/site-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of size 1 in 0.31 seconds.\n",
      "Processed batch of size 1 in 0.28 seconds.\n",
      "Processed batch of size 1 in 0.30 seconds.\n",
      "Processed batch of size 1 in 0.32 seconds.\n",
      "Processed batch of size 1 in 0.34 seconds.\n",
      "Processed batch of size 1 in 0.26 seconds.\n",
      "Processed batch of size 1 in 0.28 seconds.\n",
      "Processed batch of size 1 in 0.16 seconds.\n",
      "Processed batch of size 1 in 0.22 seconds.\n",
      "Processed batch of size 1 in 0.32 seconds.\n",
      "Processed batch of size 1 in 0.27 seconds.\n",
      "Processed batch of size 1 in 0.36 seconds.\n",
      "Processed batch of size 1 in 0.31 seconds.\n",
      "Processed batch of size 1 in 0.28 seconds.\n",
      "Processed batch of size 1 in 0.02 seconds.\n",
      "Processed batch of size 1 in 0.28 seconds.\n",
      "Processed batch of size 1 in 0.02 seconds.\n",
      "Processed batch of size 1 in 0.30 seconds.\n",
      "Processed batch of size 1 in 0.30 seconds.\n",
      "Processed batch of size 1 in 0.02 seconds.\n",
      "Processed batch of size 1 in 0.29 seconds.\n",
      "Processed batch of size 1 in 0.31 seconds.\n",
      "Processed batch of size 1 in 0.29 seconds.\n",
      "Processed batch of size 1 in 0.30 seconds.\n",
      "Processed batch of size 1 in 0.31 seconds.\n",
      "Processed batch of size 1 in 0.34 seconds.\n",
      "Processed batch of size 1 in 0.30 seconds.\n",
      "Processed batch of size 1 in 0.29 seconds.\n",
      "Processed batch of size 1 in 0.30 seconds.\n",
      "Processed batch of size 1 in 0.33 seconds.\n",
      "Processed batch of size 1 in 0.35 seconds.\n",
      "Processed batch of size 1 in 0.30 seconds.\n",
      "Processed batch of size 1 in 0.28 seconds.\n",
      "Processed batch of size 1 in 0.30 seconds.\n",
      "Processed batch of size 1 in 0.30 seconds.\n",
      "Processed batch of size 1 in 0.02 seconds.\n",
      "Processed batch of size 1 in 0.24 seconds.\n",
      "Processed batch of size 1 in 0.30 seconds.\n",
      "Processed batch of size 1 in 0.28 seconds.\n",
      "Processed batch of size 1 in 0.30 seconds.\n",
      "Processed batch of size 1 in 0.30 seconds.\n",
      "Processed batch of size 1 in 0.30 seconds.\n",
      "Processed batch of size 1 in 0.32 seconds.\n",
      "Processed batch of size 1 in 0.32 seconds.\n",
      "Processed batch of size 1 in 0.30 seconds.\n",
      "Processed batch of size 1 in 0.28 seconds.\n",
      "Processed batch of size 1 in 0.28 seconds.\n",
      "Processed batch of size 1 in 0.30 seconds.\n",
      "Processed batch of size 1 in 0.26 seconds.\n",
      "Processed batch of size 1 in 0.26 seconds.\n",
      "Processed batch of size 1 in 0.24 seconds.\n",
      "Processed batch of size 1 in 0.26 seconds.\n",
      "Processed batch of size 1 in 0.34 seconds.\n",
      "Processed batch of size 1 in 0.32 seconds.\n",
      "Processed batch of size 1 in 0.30 seconds.\n",
      "Processed batch of size 1 in 0.28 seconds.\n",
      "Processed batch of size 1 in 0.32 seconds.\n",
      "Processed batch of size 1 in 0.30 seconds.\n",
      "Processed batch of size 1 in 0.30 seconds.\n",
      "Processed batch of size 1 in 0.30 seconds.\n",
      "Processed batch of size 1 in 0.30 seconds.\n",
      "Processed batch of size 1 in 0.28 seconds.\n",
      "Processed batch of size 1 in 0.32 seconds.\n",
      "Processed batch of size 1 in 0.30 seconds.\n",
      "Processed batch of size 1 in 0.30 seconds.\n",
      "Processed batch of size 1 in 0.03 seconds.\n",
      "Processed batch of size 1 in 0.34 seconds.\n",
      "Processed batch of size 1 in 0.30 seconds.\n",
      "Total end-to-end time for batch size 1: 21.84 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  2.12it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 7.79 GiB of which 105.38 MiB is free. Including non-PyTorch memory, this process has 7.02 GiB memory in use. Of the allocated memory 6.76 GiB is allocated by PyTorch, and 93.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 77\u001b[0m\n\u001b[1;32m     73\u001b[0m             total_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time  \u001b[38;5;66;03m# Calculate total time for this batch size\u001b[39;00m\n\u001b[1;32m     74\u001b[0m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal end-to-end time for batch size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_time\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 77\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 59\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m overall_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch_size \u001b[38;5;129;01min\u001b[39;00m batch_sizes:\n\u001b[0;32m---> 59\u001b[0m         model \u001b[38;5;241m=\u001b[39m \u001b[43mModelManager\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgpt-2\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     60\u001b[0m         start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()  \u001b[38;5;66;03m# Start timing for this batch size scenario\u001b[39;00m\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m ThreadPoolExecutor(max_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m executor:  \u001b[38;5;66;03m# Adjust max_workers based on your CPU cores\u001b[39;00m\n",
      "File \u001b[0;32m~/projects/RayInference/models/model_manager.py:26\u001b[0m, in \u001b[0;36mModelManager.__init__\u001b[0;34m(self, model_name, local_path)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpipeline \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m'\u001b[39m, model\u001b[38;5;241m=\u001b[39mlocal_path, torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16)\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 26\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpipeline \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext-generation\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbfloat16\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;66;03m# self.pipeline.tokenizer.pad_token_id = self.pipeline.model.config.eos_token_id\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpipeline \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m'\u001b[39m, model\u001b[38;5;241m=\u001b[39mmodel_name, torch_dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mbfloat16)\n",
      "File \u001b[0;32m~/miniconda3/envs/ray/lib/python3.9/site-packages/transformers/pipelines/__init__.py:1107\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1105\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m device\n\u001b[0;32m-> 1107\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpipeline_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ray/lib/python3.9/site-packages/transformers/pipelines/text_generation.py:84\u001b[0m, in \u001b[0;36mTextGenerationPipeline.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 84\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_model_type(\n\u001b[1;32m     86\u001b[0m         TF_MODEL_FOR_CAUSAL_LM_MAPPING_NAMES \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m MODEL_FOR_CAUSAL_LM_MAPPING_NAMES\n\u001b[1;32m     87\u001b[0m     )\n\u001b[1;32m     88\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprefix\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preprocess_params:\n\u001b[1;32m     89\u001b[0m         \u001b[38;5;66;03m# This is very specific. The logic is quite complex and needs to be done\u001b[39;00m\n\u001b[1;32m     90\u001b[0m         \u001b[38;5;66;03m# as a \"default\".\u001b[39;00m\n\u001b[1;32m     91\u001b[0m         \u001b[38;5;66;03m# It also defines both some preprocess_kwargs and generate_kwargs\u001b[39;00m\n\u001b[1;32m     92\u001b[0m         \u001b[38;5;66;03m# which is why we cannot put them in their respective methods.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ray/lib/python3.9/site-packages/transformers/pipelines/base.py:874\u001b[0m, in \u001b[0;36mPipeline.__init__\u001b[0;34m(self, model, tokenizer, feature_extractor, image_processor, modelcard, framework, task, args_parser, device, torch_dtype, binary_output, **kwargs)\u001b[0m\n\u001b[1;32m    867\u001b[0m \u001b[38;5;66;03m# We shouldn't call `model.to()` for models loaded with accelerate\u001b[39;00m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    872\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m hf_device_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    873\u001b[0m ):\n\u001b[0;32m--> 874\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[38;5;66;03m# Update config and generation_config with task specific parameters\u001b[39;00m\n\u001b[1;32m    877\u001b[0m task_specific_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mtask_specific_params\n",
      "File \u001b[0;32m~/miniconda3/envs/ray/lib/python3.9/site-packages/transformers/modeling_utils.py:2556\u001b[0m, in \u001b[0;36mPreTrainedModel.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2551\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_present_in_args:\n\u001b[1;32m   2552\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   2553\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou cannot cast a GPTQ model in a new `dtype`. Make sure to load the model using `from_pretrained` using the desired\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2554\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m `dtype` by passing the correct `torch_dtype` argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   2555\u001b[0m         )\n\u001b[0;32m-> 2556\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ray/lib/python3.9/site-packages/torch/nn/modules/module.py:1152\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1152\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ray/lib/python3.9/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ray/lib/python3.9/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[0;31m[... skipping similar frames: Module._apply at line 802 (2 times)]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/ray/lib/python3.9/site-packages/torch/nn/modules/module.py:802\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    800\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    801\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 802\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    804\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    806\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    807\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    812\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    813\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ray/lib/python3.9/site-packages/torch/nn/modules/module.py:825\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    821\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    822\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    823\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    824\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 825\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    826\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/miniconda3/envs/ray/lib/python3.9/site-packages/torch/nn/modules/module.py:1150\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1148\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1149\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1150\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 64.00 MiB. GPU 0 has a total capacity of 7.79 GiB of which 105.38 MiB is free. Including non-PyTorch memory, this process has 7.02 GiB memory in use. Of the allocated memory 6.76 GiB is allocated by PyTorch, and 93.68 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np  # Assuming numerical operations might be needed\n",
    "from models.model_manager import ModelManager\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# class ModelManager:\n",
    "#     def __init__(self, model_path):\n",
    "#         # Dummy initialization - replace with actual model loading logic\n",
    "#         self.model_path = model_path\n",
    "    \n",
    "#     def infer(self, texts):\n",
    "#         # Dummy inference - simulate processing time and return dummy results\n",
    "#         time.sleep(0.01 * len(texts))  # Simulate inference time scaling with batch size\n",
    "#         return [\"Result for: \" + text for text in texts]\n",
    "\n",
    "def load_data(file_path):\n",
    "    \"\"\" Load data from a CSV file assuming there's a column 'prompt' for input data. \"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    return df['prompt'].tolist()\n",
    "\n",
    "def process_batches(prompts, model, batch_size):\n",
    "    \"\"\" Process data in batches and collect results. \"\"\"\n",
    "    results = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i in range(0, len(prompts), batch_size):\n",
    "        batch = prompts[i:i + batch_size]\n",
    "        batch_results = model.infer(batch)\n",
    "        results.extend(batch_results)\n",
    "        \n",
    "def process_batch(batch, model):\n",
    "    \"\"\" Process a single batch of prompts using the given model. \"\"\"\n",
    "    start_time = time.time()\n",
    "    results = model.infer(batch)\n",
    "    duration = time.time() - start_time\n",
    "    return {'results': results, 'duration': duration, 'batch_size': len(batch)}\n",
    "\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"Processed {len(prompts)} prompts in {total_time:.2f} seconds with a batch size of {batch_size}\")\n",
    "    return results\n",
    "\n",
    "def main():\n",
    "    data_path = 'test1.csv'  # Path to your CSV file\n",
    "    model_path = '/home/peiman/projects/RayInference/gemma-2'  # Path to your model, adjust accordingly\n",
    "\n",
    "    # Initialize model\n",
    "\n",
    "    # Load data\n",
    "    prompts = load_data(data_path)\n",
    "\n",
    "    # Define different batch sizes to test\n",
    "    batch_sizes = [1, 2, 4]\n",
    "\n",
    "    overall_start_time = time.time()\n",
    "\n",
    "    for batch_size in batch_sizes:\n",
    "            model = ModelManager('gpt-2', model_path)\n",
    "            start_time = time.time()  # Start timing for this batch size scenario\n",
    "            with ThreadPoolExecutor(max_workers=1) as executor:  # Adjust max_workers based on your CPU cores\n",
    "                futures = []\n",
    "                for i in range(0, len(prompts), batch_size):\n",
    "                    batch = prompts[i:i+batch_size]\n",
    "                    futures.append(executor.submit(process_batch, batch, model))\n",
    "\n",
    "                all_results = []\n",
    "                for future in as_completed(futures):\n",
    "                    result = future.result()\n",
    "                    all_results.append(result)\n",
    "                    print(f\"Processed batch of size {result['batch_size']} in {result['duration']:.2f} seconds.\")\n",
    "\n",
    "            total_time = time.time() - start_time  # Calculate total time for this batch size\n",
    "            print(f\"Total end-to-end time for batch size {batch_size}: {total_time:.2f} seconds\")\n",
    "\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of size 1 in 1.08 seconds.\n",
      "Processed batch of size 1 in 1.11 seconds.\n",
      "Processed batch of size 1 in 1.16 seconds.\n",
      "Processed batch of size 1 in 1.22 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of size 1 in 0.66 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "/home/peiman/miniconda3/envs/ray/lib/python3.9/site-packages/transformers/pipelines/base.py:1157: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
      "  warnings.warn(\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of size 1 in 1.12 seconds.\n",
      "Processed batch of size 1 in 1.16 seconds.\n",
      "Processed batch of size 1 in 1.16 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of size 1 in 1.14 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of size 1 in 1.10 seconds.\n",
      "Processed batch of size 1 in 1.14 seconds.\n",
      "Processed batch of size 1 in 1.14 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of size 1 in 1.16 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of size 1 in 1.20 seconds.\n",
      "Processed batch of size 1 in 1.24 seconds.\n",
      "Processed batch of size 1 in 1.19 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of size 1 in 1.20 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of size 1 in 1.03 seconds.\n",
      "Processed batch of size 1 in 1.21 seconds.\n",
      "Processed batch of size 1 in 1.13 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of size 1 in 1.18 seconds.\n",
      "Processed batch of size 1 in 1.06 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of size 1 in 1.16 seconds.\n",
      "Processed batch of size 1 in 1.22 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of size 1 in 1.16 seconds.\n",
      "Processed batch of size 1 in 1.16 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of size 1 in 1.13 seconds.\n",
      "Processed batch of size 1 in 1.15 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of size 1 in 0.58 seconds.\n",
      "Processed batch of size 1 in 1.11 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of size 1 in 1.12 seconds.\n",
      "Processed batch of size 1 in 1.06 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of size 1 in 1.23 seconds.\n",
      "Processed batch of size 1 in 1.09 seconds.\n",
      "Processed batch of size 1 in 1.09 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of size 1 in 1.19 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of size 1 in 1.17 seconds.\n",
      "Processed batch of size 1 in 1.22 seconds.\n",
      "Processed batch of size 1 in 1.17 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of size 1 in 1.17 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of size 1 in 0.82 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of size 1 in 1.20 seconds.\n",
      "Processed batch of size 1 in 1.13 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of size 1 in 1.11 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of size 1 in 1.15 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of size 1 in 1.18 seconds.\n",
      "Processed batch of size 1 in 1.12 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of size 1 in 1.14 seconds.\n",
      "Processed batch of size 1 in 0.03 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of size 1 in 1.19 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of size 1 in 1.19 seconds.\n",
      "Processed batch of size 1 in 1.23 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of size 1 in 1.25 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of size 1 in 1.19 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of size 1 in 1.18 seconds.\n",
      "Processed batch of size 1 in 1.16 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of size 1 in 1.10 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of size 1 in 1.12 seconds.\n",
      "Processed batch of size 1 in 0.03 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of size 1 in 1.09 seconds.\n",
      "Processed batch of size 1 in 1.18 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of size 1 in 1.14 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of size 1 in 1.14 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of size 1 in 1.22 seconds.\n",
      "Processed batch of size 1 in 1.24 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of size 1 in 1.25 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of size 1 in 1.08 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of size 1 in 1.22 seconds.\n",
      "Processed batch of size 1 in 1.12 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of size 1 in 1.16 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of size 1 in 1.17 seconds.\n",
      "Processed batch of size 1 in 0.10 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of size 1 in 1.15 seconds.\n",
      "Processed batch of size 1 in 1.15 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of size 1 in 1.20 seconds.\n",
      "Processed batch of size 1 in 1.17 seconds.\n",
      "Processed batch of size 1 in 0.98 seconds.\n",
      "Processed batch of size 1 in 1.00 seconds.\n",
      "Processed batch of size 1 in 0.76 seconds.\n",
      "Total end-to-end time for batch size 1: 21.64 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of size 2 in 1.10 seconds.\n",
      "Processed batch of size 2 in 1.16 seconds.\n",
      "Processed batch of size 2 in 1.18 seconds.\n",
      "Processed batch of size 2 in 1.25 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of size 2 in 1.15 seconds.\n",
      "Processed batch of size 2 in 1.12 seconds.\n",
      "Processed batch of size 2 in 1.18 seconds.\n",
      "Processed batch of size 2 in 1.18 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of size 2 in 1.05 seconds.\n",
      "Processed batch of size 2 in 1.15 seconds.\n",
      "Processed batch of size 2 in 1.11 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of size 2 in 1.19 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of size 2 in 1.12 seconds.\n",
      "Processed batch of size 2 in 1.12 seconds.\n",
      "Processed batch of size 2 in 1.14 seconds.\n",
      "Processed batch of size 2 in 1.05 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of size 2 in 1.12 seconds.\n",
      "Processed batch of size 2 in 1.14 seconds.\n",
      "Processed batch of size 2 in 1.14 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of size 2 in 1.13 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of size 2 in 1.17 seconds.\n",
      "Processed batch of size 2 in 1.26 seconds.\n",
      "Processed batch of size 2 in 1.16 seconds.\n",
      "Processed batch of size 2 in 1.09 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of size 2 in 1.13 seconds.\n",
      "Processed batch of size 2 in 1.16 seconds.\n",
      "Processed batch of size 2 in 1.13 seconds.\n",
      "Processed batch of size 2 in 1.17 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of size 2 in 1.07 seconds.\n",
      "Processed batch of size 2 in 1.09 seconds.\n",
      "Processed batch of size 2 in 1.21 seconds.\n",
      "Processed batch of size 2 in 1.16 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of size 2 in 1.10 seconds.\n",
      "Processed batch of size 2 in 1.16 seconds.\n",
      "Processed batch of size 2 in 1.12 seconds.\n",
      "Processed batch of size 2 in 1.15 seconds.\n",
      "Processed batch of size 2 in 1.11 seconds.\n",
      "Processed batch of size 2 in 1.12 seconds.\n",
      "Processed batch of size 2 in 1.10 seconds.\n",
      "Processed batch of size 1 in 1.03 seconds.\n",
      "Total end-to-end time for batch size 2: 11.39 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of size 4 in 1.15 seconds.\n",
      "Processed batch of size 4 in 1.16 seconds.\n",
      "Processed batch of size 4 in 1.21 seconds.\n",
      "Processed batch of size 4 in 1.23 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of size 4 in 1.07 seconds.\n",
      "Processed batch of size 4 in 1.13 seconds.\n",
      "Processed batch of size 4 in 1.12 seconds.\n",
      "Processed batch of size 4 in 1.18 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of size 4 in 1.21 seconds.\n",
      "Processed batch of size 4 in 1.11 seconds.\n",
      "Processed batch of size 4 in 1.18 seconds.\n",
      "Processed batch of size 4 in 1.14 seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed batch of size 4 in 1.09 seconds.\n",
      "Processed batch of size 4 in 1.16 seconds.\n",
      "Processed batch of size 4 in 1.10 seconds.\n",
      "Processed batch of size 4 in 1.18 seconds.\n",
      "Processed batch of size 4 in 1.12 seconds.\n",
      "Processed batch of size 3 in 1.08 seconds.\n",
      "Processed batch of size 4 in 1.17 seconds.\n",
      "Processed batch of size 4 in 1.15 seconds.\n",
      "Total end-to-end time for batch size 4: 5.78 seconds\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np  # Assuming numerical operations might be needed\n",
    "from models.model_managerV2 import ModelManager\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# class ModelManager:\n",
    "#     def __init__(self, model_path):\n",
    "#         # Dummy initialization - replace with actual model loading logic\n",
    "#         self.model_path = model_path\n",
    "    \n",
    "#     def infer(self, texts):\n",
    "#         # Dummy inference - simulate processing time and return dummy results\n",
    "#         time.sleep(0.01 * len(texts))  # Simulate inference time scaling with batch size\n",
    "#         return [\"Result for: \" + text for text in texts]\n",
    "\n",
    "def load_data(file_path):\n",
    "    \"\"\" Load data from a CSV file assuming there's a column 'prompt' for input data. \"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    return df['prompt'].tolist()\n",
    "\n",
    "def process_batches(prompts, model, batch_size):\n",
    "    \"\"\" Process data in batches and collect results. \"\"\"\n",
    "    results = []\n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i in range(0, len(prompts), batch_size):\n",
    "        batch = prompts[i:i + batch_size]\n",
    "        batch_results = model.infer(batch)\n",
    "        results.extend(batch_results)\n",
    "        \n",
    "def process_batch(batch, model):\n",
    "    \"\"\" Process a single batch of prompts using the given model. \"\"\"\n",
    "    start_time = time.time()\n",
    "    results = model.infer(batch)\n",
    "    duration = time.time() - start_time\n",
    "    return {'results': results, 'duration': duration, 'batch_size': len(batch)}\n",
    "\n",
    "    \n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"Processed {len(prompts)} prompts in {total_time:.2f} seconds with a batch size of {batch_size}\")\n",
    "    return results\n",
    "\n",
    "def main():\n",
    "    data_path = 'test1.csv'  # Path to your CSV file\n",
    "    model_path = '/home/peiman/projects/RayInference/gemma-2'  # Path to your model, adjust accordingly\n",
    "\n",
    "    # Initialize model\n",
    "\n",
    "    # Load data\n",
    "    prompts = load_data(data_path)\n",
    "\n",
    "    # Define different batch sizes to test\n",
    "    batch_sizes = [1, 2, 4]\n",
    "\n",
    "    overall_start_time = time.time()\n",
    "\n",
    "    for batch_size in batch_sizes:\n",
    "            model = ModelManager('gpt-2', model_path, batch_size)\n",
    "            start_time = time.time()  # Start timing for this batch size scenario\n",
    "            with ThreadPoolExecutor(max_workers=4) as executor:  # Adjust max_workers based on your CPU cores\n",
    "                futures = []\n",
    "                for i in range(0, len(prompts), batch_size):\n",
    "                    batch = prompts[i:i+batch_size]\n",
    "                    futures.append(executor.submit(process_batch, batch, model))\n",
    "\n",
    "                all_results = []\n",
    "                for future in as_completed(futures):\n",
    "                    result = future.result()\n",
    "                    all_results.append(result)\n",
    "                    print(f\"Processed batch of size {result['batch_size']} in {result['duration']:.2f} seconds.\")\n",
    "\n",
    "            total_time = time.time() - start_time  # Calculate total time for this batch size\n",
    "            print(f\"Total end-to-end time for batch size {batch_size}: {total_time:.2f} seconds\")\n",
    "\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base batch size 142 does not distribute data evenly among 7 actors.\n",
      "Base batch size 141 does not distribute data evenly among 7 actors.\n",
      "Base batch size 140 does not distribute data evenly among 7 actors.\n",
      "Base batch size 139 does not distribute data evenly among 7 actors.\n",
      "Base batch size 138 does not distribute data evenly among 7 actors.\n",
      "Base batch size 137 does not distribute data evenly among 7 actors.\n",
      "Base batch size 136 does not distribute data evenly among 7 actors.\n",
      "Base batch size 135 does not distribute data evenly among 7 actors.\n",
      "Base batch size 134 does not distribute data evenly among 7 actors.\n",
      "Base batch size 133 does not distribute data evenly among 7 actors.\n",
      "Base batch size 132 does not distribute data evenly among 7 actors.\n",
      "Base batch size 131 does not distribute data evenly among 7 actors.\n",
      "Base batch size 130 does not distribute data evenly among 7 actors.\n",
      "Base batch size 129 does not distribute data evenly among 7 actors.\n",
      "Base batch size 128 does not distribute data evenly among 7 actors.\n",
      "Base batch size 127 does not distribute data evenly among 7 actors.\n",
      "Base batch size 126 does not distribute data evenly among 7 actors.\n",
      "Base batch size 125 does not distribute data evenly among 7 actors.\n",
      "Base batch size 124 does not distribute data evenly among 7 actors.\n",
      "Base batch size 123 does not distribute data evenly among 7 actors.\n",
      "Base batch size 122 does not distribute data evenly among 7 actors.\n",
      "Base batch size 121 does not distribute data evenly among 7 actors.\n",
      "Base batch size 120 does not distribute data evenly among 7 actors.\n",
      "Base batch size 119 does not distribute data evenly among 7 actors.\n",
      "Base batch size 118 does not distribute data evenly among 7 actors.\n",
      "Base batch size 117 does not distribute data evenly among 7 actors.\n",
      "Base batch size 116 does not distribute data evenly among 7 actors.\n",
      "Base batch size 115 does not distribute data evenly among 7 actors.\n",
      "Base batch size 114 does not distribute data evenly among 7 actors.\n",
      "Base batch size 113 does not distribute data evenly among 7 actors.\n",
      "Base batch size 112 does not distribute data evenly among 7 actors.\n",
      "Base batch size 111 does not distribute data evenly among 7 actors.\n",
      "Base batch size 110 does not distribute data evenly among 7 actors.\n",
      "Base batch size 109 does not distribute data evenly among 7 actors.\n",
      "Base batch size 108 does not distribute data evenly among 7 actors.\n",
      "Base batch size 107 does not distribute data evenly among 7 actors.\n",
      "Base batch size 106 does not distribute data evenly among 7 actors.\n",
      "Base batch size 105 does not distribute data evenly among 7 actors.\n",
      "Base batch size 104 does not distribute data evenly among 7 actors.\n",
      "Base batch size 103 does not distribute data evenly among 7 actors.\n",
      "Base batch size 102 does not distribute data evenly among 7 actors.\n",
      "Base batch size 101 does not distribute data evenly among 7 actors.\n",
      "Base batch size 100 does not distribute data evenly among 7 actors.\n",
      "Base batch size 99 does not distribute data evenly among 7 actors.\n",
      "Base batch size 98 does not distribute data evenly among 7 actors.\n",
      "Base batch size 97 does not distribute data evenly among 7 actors.\n",
      "Base batch size 96 does not distribute data evenly among 7 actors.\n",
      "Base batch size 95 does not distribute data evenly among 7 actors.\n",
      "Base batch size 94 does not distribute data evenly among 7 actors.\n",
      "Base batch size 93 does not distribute data evenly among 7 actors.\n",
      "Base batch size 92 does not distribute data evenly among 7 actors.\n",
      "Base batch size 91 does not distribute data evenly among 7 actors.\n",
      "Base batch size 90 does not distribute data evenly among 7 actors.\n",
      "Base batch size 89 does not distribute data evenly among 7 actors.\n",
      "Base batch size 88 does not distribute data evenly among 7 actors.\n",
      "Base batch size 87 does not distribute data evenly among 7 actors.\n",
      "Base batch size 86 does not distribute data evenly among 7 actors.\n",
      "Base batch size 85 does not distribute data evenly among 7 actors.\n",
      "Base batch size 84 does not distribute data evenly among 7 actors.\n",
      "Base batch size 83 does not distribute data evenly among 7 actors.\n",
      "Base batch size 82 does not distribute data evenly among 7 actors.\n",
      "Base batch size 81 does not distribute data evenly among 7 actors.\n",
      "Base batch size 80 does not distribute data evenly among 7 actors.\n",
      "Base batch size 79 does not distribute data evenly among 7 actors.\n",
      "Base batch size 78 does not distribute data evenly among 7 actors.\n",
      "Base batch size 77 does not distribute data evenly among 7 actors.\n",
      "Base batch size 76 does not distribute data evenly among 7 actors.\n",
      "Base batch size 75 does not distribute data evenly among 7 actors.\n",
      "Base batch size 74 does not distribute data evenly among 7 actors.\n",
      "Base batch size 73 does not distribute data evenly among 7 actors.\n",
      "Base batch size 72 does not distribute data evenly among 7 actors.\n",
      "Base batch size 71 does not distribute data evenly among 7 actors.\n",
      "Base batch size 70 does not distribute data evenly among 7 actors.\n",
      "Base batch size 69 does not distribute data evenly among 7 actors.\n",
      "Base batch size 68 does not distribute data evenly among 7 actors.\n",
      "Base batch size 67 does not distribute data evenly among 7 actors.\n",
      "Base batch size 66 does not distribute data evenly among 7 actors.\n",
      "Base batch size 65 does not distribute data evenly among 7 actors.\n",
      "Base batch size 64 does not distribute data evenly among 7 actors.\n",
      "Base batch size 63 does not distribute data evenly among 7 actors.\n",
      "Base batch size 62 does not distribute data evenly among 7 actors.\n",
      "Base batch size 61 does not distribute data evenly among 7 actors.\n",
      "Base batch size 60 does not distribute data evenly among 7 actors.\n",
      "Base batch size 59 does not distribute data evenly among 7 actors.\n",
      "Base batch size 58 does not distribute data evenly among 7 actors.\n",
      "Base batch size 57 does not distribute data evenly among 7 actors.\n",
      "Base batch size 56 does not distribute data evenly among 7 actors.\n",
      "Base batch size 55 does not distribute data evenly among 7 actors.\n",
      "Base batch size 54 does not distribute data evenly among 7 actors.\n",
      "Base batch size 53 does not distribute data evenly among 7 actors.\n",
      "Base batch size 52 does not distribute data evenly among 7 actors.\n",
      "Base batch size 51 does not distribute data evenly among 7 actors.\n",
      "Base batch size 50 does not distribute data evenly among 7 actors.\n",
      "Base batch size 49 does not distribute data evenly among 7 actors.\n",
      "Base batch size 48 does not distribute data evenly among 7 actors.\n",
      "Base batch size 47 does not distribute data evenly among 7 actors.\n",
      "Base batch size 46 does not distribute data evenly among 7 actors.\n",
      "Base batch size 45 does not distribute data evenly among 7 actors.\n",
      "Base batch size 44 does not distribute data evenly among 7 actors.\n",
      "Base batch size 43 does not distribute data evenly among 7 actors.\n",
      "Base batch size 42 does not distribute data evenly among 7 actors.\n",
      "Base batch size 41 does not distribute data evenly among 7 actors.\n",
      "Base batch size 40 does not distribute data evenly among 7 actors.\n",
      "Base batch size 39 does not distribute data evenly among 7 actors.\n",
      "Base batch size 38 does not distribute data evenly among 7 actors.\n",
      "Base batch size 37 does not distribute data evenly among 7 actors.\n",
      "Base batch size 36 does not distribute data evenly among 7 actors.\n",
      "Base batch size 35 does not distribute data evenly among 7 actors.\n",
      "Base batch size 34 does not distribute data evenly among 7 actors.\n",
      "Base batch size 33 does not distribute data evenly among 7 actors.\n",
      "Base batch size 32 does not distribute data evenly among 7 actors.\n",
      "Base batch size 31 does not distribute data evenly among 7 actors.\n",
      "Base batch size 30 does not distribute data evenly among 7 actors.\n",
      "Base batch size 29 does not distribute data evenly among 7 actors.\n",
      "Base batch size 28 does not distribute data evenly among 7 actors.\n",
      "Base batch size 27 does not distribute data evenly among 7 actors.\n",
      "Base batch size 26 does not distribute data evenly among 7 actors.\n",
      "Base batch size 25 does not distribute data evenly among 7 actors.\n",
      "Base batch size 24 does not distribute data evenly among 7 actors.\n",
      "Base batch size 23 does not distribute data evenly among 7 actors.\n",
      "Base batch size 22 does not distribute data evenly among 7 actors.\n",
      "Base batch size 21 does not distribute data evenly among 7 actors.\n",
      "Base batch size 20 does not distribute data evenly among 7 actors.\n",
      "Base batch size 19 does not distribute data evenly among 7 actors.\n",
      "Base batch size 18 does not distribute data evenly among 7 actors.\n",
      "Base batch size 17 does not distribute data evenly among 7 actors.\n",
      "Base batch size 16 does not distribute data evenly among 7 actors.\n",
      "Base batch size 15 does not distribute data evenly among 7 actors.\n",
      "Base batch size 14 does not distribute data evenly among 7 actors.\n",
      "Base batch size 13 does not distribute data evenly among 7 actors.\n",
      "Base batch size 12 does not distribute data evenly among 7 actors.\n",
      "Base batch size 11 does not distribute data evenly among 7 actors.\n",
      "Base batch size 10 does not distribute data evenly among 7 actors.\n",
      "Base batch size 9 does not distribute data evenly among 7 actors.\n",
      "Base batch size 8 does not distribute data evenly among 7 actors.\n",
      "Base batch size 7 does not distribute data evenly among 7 actors.\n",
      "Base batch size 6 does not distribute data evenly among 7 actors.\n",
      "Base batch size 5 does not distribute data evenly among 7 actors.\n",
      "Base batch size 4 does not distribute data evenly among 7 actors.\n",
      "Base batch size 3 does not distribute data evenly among 7 actors.\n",
      "Base batch size 2 does not distribute data evenly among 7 actors.\n",
      "Base batch size 1 does not distribute data evenly among 7 actors.\n"
     ]
    },
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1000\u001b[39m\n\u001b[1;32m     16\u001b[0m m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m7\u001b[39m\n\u001b[0;32m---> 17\u001b[0m optimal_batch_size \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_optimal_batch_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mm\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimal Batch Size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptimal_batch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[15], line 9\u001b[0m, in \u001b[0;36mcalculate_optimal_batch_size\u001b[0;34m(n, m)\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m base_batch_size\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# Find the next batch size that equally distributes batches among actors\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m (\u001b[43mn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbase_batch_size\u001b[49m) \u001b[38;5;241m%\u001b[39m m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m     10\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBase batch size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbase_batch_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not distribute data evenly among \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mm\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m actors.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     11\u001b[0m         base_batch_size \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "def calculate_optimal_batch_size(n, m):\n",
    "    # Start by dividing data equally among actors\n",
    "    base_batch_size = n // m\n",
    "    # Check if this base size distributes all data evenly\n",
    "    if n % m == 0:\n",
    "        return base_batch_size\n",
    "    else:\n",
    "        # Find the next batch size that equally distributes batches among actors\n",
    "        while (n / base_batch_size) % m != 0:\n",
    "            print(f\"Base batch size {base_batch_size} does not distribute data evenly among {m} actors.\")\n",
    "            base_batch_size -= 1\n",
    "        return base_batch_size\n",
    "\n",
    "# Example usage\n",
    "n = 1000\n",
    "m = 7\n",
    "optimal_batch_size = calculate_optimal_batch_size(n, m)\n",
    "print(f\"Optimal Batch Size: {optimal_batch_size}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ray",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
